\documentclass[12pt]{article}
%\usepackage[finnish]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{delarray,amsmath,bbm,epsfig,slashed}
\usepackage{bbold}
\usepackage{listings}
\usepackage{qcircuit}
\newcommand{\pat}{\partial}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\abf}{{\bf a}}
\newcommand{\Zmath}{\mathbf{Z}}
\newcommand{\Zcal}{{\cal Z}_{12}}
\newcommand{\zcal}{z_{12}}
\newcommand{\Acal}{{\cal A}}
\newcommand{\Fcal}{{\cal F}}
\newcommand{\Ucal}{{\cal U}}
\newcommand{\Vcal}{{\cal V}}
\newcommand{\Ocal}{{\cal O}}
\newcommand{\Rcal}{{\cal R}}
\newcommand{\Scal}{{\cal S}}
\newcommand{\Lcal}{{\cal L}}
\newcommand{\Hcal}{{\cal H}}
\newcommand{\hsf}{{\sf h}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\xibar}{\bar{\xi }}
\newcommand{\barh}{\bar{h}}
\newcommand{\Ubar}{\bar{\cal U}}
\newcommand{\Vbar}{\bar{\cal V}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\zbar}{\bar{z}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\zbarhat}{\hat{\bar{z}}}
\newcommand{\wbarhat}{\hat{\bar{w}}}
\newcommand{\wbartilde}{\tilde{\bar{w}}}
\newcommand{\barone}{\bar{1}}
\newcommand{\bartwo}{\bar{2}}
\newcommand{\nbyn}{N \times N}
\newcommand{\repres}{\leftrightarrow}
\newcommand{\Tr}{{\rm Tr}}
\newcommand{\tr}{{\rm tr}}
\newcommand{\ninfty}{N \rightarrow \infty}
\newcommand{\unitk}{{\bf 1}_k}
\newcommand{\unitm}{{\bf 1}}
\newcommand{\zerom}{{\bf 0}}
\newcommand{\unittwo}{{\bf 1}_2}
\newcommand{\holo}{{\cal U}}
%\newcommand{\bra}{\langle}
%\newcommand{\ket}{\rangle}
\newcommand{\muhat}{\hat{\mu}}
\newcommand{\nuhat}{\hat{\nu}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\phat}{\hat{\phi}}
\newcommand{\that}{\hat{t}}
\newcommand{\shat}{\hat{s}}
\newcommand{\zhat}{\hat{z}}
\newcommand{\what}{\hat{w}}
\newcommand{\sgamma}{\sqrt{\gamma}}
\newcommand{\bfE}{{\bf E}}
\newcommand{\bfB}{{\bf B}}
\newcommand{\bfM}{{\bf M}}
\newcommand{\cl} {\cal l}
\newcommand{\ctilde}{\tilde{\chi}}
\newcommand{\ttilde}{\tilde{t}}
\newcommand{\ptilde}{\tilde{\phi}}
\newcommand{\utilde}{\tilde{u}}
\newcommand{\vtilde}{\tilde{v}}
\newcommand{\wtilde}{\tilde{w}}
\newcommand{\ztilde}{\tilde{z}}

% David Weir's macros


\newcommand{\nn}{\nonumber}
\newcommand{\com}[2]{\left[{#1},{#2}\right]}
\newcommand{\mrm}[1] {{\mathrm{#1}}}
\newcommand{\mbf}[1] {{\mathbf{#1}}}
\newcommand{\ave}[1]{\left\langle{#1}\right\rangle}
\newcommand{\halft}{{\textstyle \frac{1}{2}}}
\newcommand{\ie}{{\it i.e.\ }}
\newcommand{\eg}{{\it e.g.\ }}
\newcommand{\cf}{{\it cf.\ }}
\newcommand{\etal}{{\it et al.}}
\newcommand{\ket}[1]{\vert{#1}\rangle}
\newcommand{\bra}[1]{\langle{#1}\vert}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\xv}{{\bs{x}}}
\newcommand{\yv}{{\bs{y}}}
\newcommand{\pv}{{\bs{p}}}
\newcommand{\kv}{{\bs{k}}}
\newcommand{\qv}{{\bs{q}}}
\newcommand{\bv}{{\bs{b}}}
\newcommand{\ev}{{\bs{e}}}
\newcommand{\gv}{\bs{\gamma}}
\newcommand{\lv}{{\bs{\ell}}}
\newcommand{\nabv}{{\bs{\nabla}}}
\newcommand{\sigv}{{\bs{\sigma}}}
\newcommand{\notvec}{\bs{0}_\perp}
\newcommand{\inv}[1]{\frac{1}{#1}}
%\newcommand{\xv}{{\bs{x}}}
%\newcommand{\yv}{{\bs{y}}}
\newcommand{\Av}{\bs{A}}
%\newcommand{\lv}{{\bs{\ell}}}
\newcommand{\Rmath}{\mbf{R}}


%\newcommand\bsigma{\vec{\sigma}}
\hoffset 0.5cm
\voffset -0.4cm
\evensidemargin -0.2in
\oddsidemargin -0.2in
\topmargin -0.2in
\textwidth 6.3in
\textheight 8.4in

\begin{document}

\normalsize

\baselineskip 14pt

\begin{center}
{\Large {\bf Problem Set 9 \ \ Statistical Methods}} \\
Jake Muff \\
18/11/20
\end{center}

\section{Question 1}

\begin{enumerate}
    \item Covariance of $y_1$ and $y_2$. Note that $y_2$ can be rewritten 
    $$ y_2 = \frac{1}{m} \sum_{j=n-c+1}^{m+n-c} x_j = \frac{1}{m} \sum_{j=1}^{m} x_j $$
    So we have 
    $$ cov[y_1, y_2] = E \big[ ( \frac{1}{n} \sum_{i=1}^n x_i ) ( \frac{1}{m} \frac{1}{m} \sum_{j=1}^{m} x_j)] - \mu^2 $$
    $$ = \frac{1}{nm} \Big[ (nm -n-m)\mu^2 + \mu^2(n+m) + c\sigma^2]-\mu^2 $$
    $$ = \frac{c \sigma^2}{nm} $$
    This method is adapted from Glen cowan's book equation 5.13. The difference arises from the $c$ term which is introduced as an overlap between the observations. I was unsure how to calculate this for the given summation in the question, I just transformed them into more manageable ones. I haven't shown the equality between the different summations for $y_2$ however this is trivial. 
    \\
    From this we can also see that the correlation coefficient is 
    $$ \rho = \frac{c}{\sqrt{nm}} $$
    $$ \sigma_1 = \frac{\sigma}{\sqrt{n}} $$
    $$ \sigma_2 = \frac{\sigma}{\sqrt{m}} $$


    \item The weighted average for correlated results (section 7.6 GC) is
    $$ \hat{\lambda} = w y_1 + (1-w) y_2 $$
    Where the weights $w$ is 
    $$ w =\frac{\sigma^2 - \rho \sigma_1 \sigma_2}{2 \sigma_2 - 2 \rho \sigma_1 \sigma_2 } = \frac{1}{2} $$
    So we have 
    $$ \hat{\lambda} = \frac{1}{2} y_1 + \frac{1}{2} y_2 = \frac{y1 + y_2}{2} $$
    The variance is given by 
    $$ V [ \hat{\lambda} ] = \frac{1}{\frac{1}{V[y_1]} + \frac{1}{V[y_2]}} = \frac{\sigma^2}{2} $$

    \item The weighted $w_i$ are given by 
    $$ \frac{ \sum_{i,j=1}^N ( V^{-1})_{ij} }{\sum_{k,l=1}^N (V^{-1})_{kl} } $$
    The weights will always be positive because 
    $$ \frac{\sigma_1}{\sigma_2} = \sqrt{\frac{m}{n}} $$
    So the correlation coefficient will always be less than or equal to the difference between the variances. The average will always be between $y_1$ and $y_2$. 


\end{enumerate}


\end{document}